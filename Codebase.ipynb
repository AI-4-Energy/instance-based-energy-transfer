{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "58c9f978-7fce-44fe-ba7b-fd635dcfa31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Set global plotting style and create figures directory\n",
    "# Settings\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Create figures folder\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "# Folder path\n",
    "data_path = \"ELAD_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc082457-7187-4153-9a34-403b2b20f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63893b6e-ac04-41bc-8e1e-776a36585178",
   "metadata": {},
   "source": [
    "### Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8803dd5a-3d2f-4d30-b833-9c5e3a0ad9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load environmental data from CSV files\n",
    "\n",
    "# Load environmental data\n",
    "weather = pd.read_csv(os.path.join(data_path, \"weather.csv\"))\n",
    "carbon = pd.read_csv(os.path.join(data_path, \"carbon_intensity.csv\"))\n",
    "env_data = pd.concat([weather, carbon], axis=1)\n",
    "\n",
    "# Load all building data\n",
    "buildings = []\n",
    "building_names = []\n",
    "for i in range(1, 18):  # Building_1 to Building_17\n",
    "    file_name = f\"Building_{i}.csv\"\n",
    "    full_path = os.path.join(data_path, file_name)\n",
    "    df = pd.read_csv(full_path)\n",
    "    df_full = pd.concat([df, env_data.copy()], axis=1)\n",
    "    buildings.append(df_full)\n",
    "    building_names.append(f\"Building_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5cf330-2827-44de-8424-eb03da095562",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d6f5a-3356-44cf-9914-999272b51954",
   "metadata": {},
   "outputs": [],
   "source": [
    "dhw_means = []\n",
    "features_for_distance = []\n",
    "\n",
    "for i in range(len(buildings)):\n",
    "    name = building_names[i]\n",
    "    print(f\"\\nProcessing {name}\")\n",
    "\n",
    "    df_clean = clean_and_impute(buildings[i])\n",
    "\n",
    "    # Save preprocessed CSV\n",
    "    df_clean.to_csv(f\"Dataset_Preprocessed/{name}.csv\", index=False)\n",
    "\n",
    "    # Plots\n",
    "    plot_heatmap(df_clean, name)\n",
    "    plot_each_column(df_clean, name)\n",
    "\n",
    "    # Store for clustering\n",
    "    if 'non_shiftable_load' in df_clean.columns:\n",
    "        dhw_means.append(df_clean['non_shiftable_load'].mean())\n",
    "    else:\n",
    "        dhw_means.append(0)  # or np.nan if preferred\n",
    "\n",
    "    features_for_distance.append(df_clean.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2016e-baf9-40fe-8196-fb2d8e86bf26",
   "metadata": {},
   "source": [
    "#### CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9caabd12-4591-4c1b-815e-b75251a55312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# Format labels nicely\n",
    "pretty_names = [name.replace(\"_\", \" \") for name in building_names]\n",
    "\n",
    "# --- Clustering ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "linked = linkage(np.array(non_shiftable_means).reshape(-1, 1), method='ward')\n",
    "\n",
    "# Dendrogram with clean labels\n",
    "ddata = dendrogram(\n",
    "    linked,\n",
    "    labels=pretty_names,\n",
    "    orientation='top',\n",
    "    color_threshold=0.6\n",
    ")\n",
    "\n",
    "# Thicken colored lines\n",
    "ax = plt.gca()\n",
    "for collection in ax.collections:\n",
    "    collection.set_linewidth(4)\n",
    "\n",
    "plt.xticks(rotation=90, fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/Clustering/hierarchical/Hierarical_Clustering.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88450d9-04f0-4316-aaf2-e89d3160e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Inputs: building_names = list of names like [\"Building_1\", ..., \"Building_17\"]\n",
    "#         buildings = list of corresponding cleaned DataFrames\n",
    "\n",
    "# --- 2.1 Hierarchical Clustering on mean non_shiftable_load ---\n",
    "non_shiftable_means = []\n",
    "features_for_distance = []\n",
    "\n",
    "for df in buildings:\n",
    "    if 'non_shiftable_load' in df.columns:\n",
    "        non_shiftable_means.append(df['non_shiftable_load'].mean())\n",
    "    else:\n",
    "        non_shiftable_means.append(0)  # or np.nan\n",
    "\n",
    "    features_for_distance.append(df.mean(numeric_only=True))\n",
    "\n",
    "# Hierarchical Clustering (non_shiftable_load)\n",
    "plt.figure(figsize=(12, 6))  # You can increase width if still crowded\n",
    "linked = linkage(np.array(non_shiftable_means).reshape(-1, 1), method='ward')\n",
    "dendrogram(linked, labels=building_names, orientation='top')\n",
    "plt.xticks(rotation=90)  # Rotate x labels\n",
    "plt.title(\"Hierarchical Clustering (non_shiftable_load Mean)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/Clustering/hierarchical/Hierarchical_Clustering.png\")\n",
    "plt.close()\n",
    "\n",
    "distance_threshold = 0.3  \n",
    "hierarchical_labels = fcluster(linked, t=5, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c5b7b-e3ce-4a8c-9849-c07101aa65e8",
   "metadata": {},
   "source": [
    "# Clustering Results:\n",
    "\n",
    "#### Cluster 1: 15, 3, 9\n",
    "#### Cluster 2: 7, 14, 2, 5, 8\n",
    "#### Cluster 3: 12, 16\n",
    "#### Cluster 4: 4, 13, 1, 6\n",
    "#### Cluster 5: 17, 10, 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34baa77-0f00-4a66-b6d1-c8477a1159ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_datetime(df):\n",
    "    df['hour'] = df['hour'].replace(24, 0)\n",
    "    df['day'] = (df.index // 24) + 1\n",
    "    df['year'] = 2022\n",
    "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']], errors='coerce')\n",
    "    df.drop(columns=['year'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4d0d1-52e1-4ff7-9b5f-06054911665c",
   "metadata": {},
   "source": [
    "### XGBOOST \n",
    "\n",
    "#### 24h Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7675574-06fa-447c-b0ab-feac3aeeefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load environmental data from CSV files\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Configuration\n",
    "data_path = \"Dataset_Preprocessed\"\n",
    "building_files = sorted([f for f in os.listdir(data_path) if f.startswith(\"Building_\")])\n",
    "forecast_horizon = 24\n",
    "target_col = 'non_shiftable_load'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"models/xgboost\", exist_ok=True)\n",
    "\n",
    "# Expanded hyperparameter grid for thorough HPO\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Function to add lag and rolling window features\n",
    "def add_lag_and_rolling_features(df, target_col='non_shiftable_load', lags=[1, 2, 3, 6, 12, 24], roll_windows=[6, 12, 24]):\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    for window in roll_windows:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "    return df\n",
    "\n",
    "# Function to construct datetime from month, hour, and day_type\n",
    "def construct_datetime(df):\n",
    "    # Replace hour 24 with 0 and adjust day_type accordingly\n",
    "    df['hour'] = df['hour'].replace(24, 0)\n",
    "    df['day_type'] = np.where(df['hour'] == 0, df['day_type'] + 1, df['day_type'])\n",
    "    df['day_type'] = df['day_type'].replace(8, 1)  # Wrap around to Monday if day_type was Sunday\n",
    "\n",
    "    # Assume the data starts from the first day of the month\n",
    "    df['day'] = (df.index // 24) + 1\n",
    "\n",
    "    # Construct datetime\n",
    "    df['year'] = 2022\n",
    "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']], errors='coerce')\n",
    "    df.drop(columns=['year'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "# Process each building file\n",
    "for file in building_files:\n",
    "    df = pd.read_csv(os.path.join(data_path, file))\n",
    "\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Skipping {file} ‚Äî no target column '{target_col}'.\")\n",
    "        continue\n",
    "\n",
    "    # Construct accurate datetime\n",
    "    df = construct_datetime(df)\n",
    "\n",
    "    # Feature engineering\n",
    "    df = add_lag_and_rolling_features(df, target_col=target_col)\n",
    "\n",
    "    # Shift target for forecasting\n",
    "    df['target'] = df[target_col].shift(-forecast_horizon)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Define feature columns\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col, 'target', 'datetime', 'month', 'day', 'hour', 'day_type']]\n",
    "\n",
    "    # Prepare input and output data\n",
    "    X = df[feature_cols].values\n",
    "    y = df['target'].values\n",
    "\n",
    "    # Time-based train-test split\n",
    "    split_idx = int(len(X) * 0.7)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    time_test = df[['datetime', 'month', 'day', 'hour']].iloc[split_idx:]\n",
    "\n",
    "    # Hyperparameter optimization and model training\n",
    "    model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    grid = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Retrieve the best model\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # Save the trained model\n",
    "    model_filename = f\"models/xgboost/{file.replace('.csv', '')}_xgb_model.pkl\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"Building\": file.replace(\".csv\", \"\"),\n",
    "        \"Best_Params\": grid.best_params_,\n",
    "        \"RMSE_24h\": rmse,\n",
    "        \"MAE_24h\": mae\n",
    "    })\n",
    "\n",
    "    print(f\"‚úÖ {file}: RMSE={rmse:.2f}, MAE={mae:.2f}\")\n",
    "\n",
    "    # Save detailed predictions\n",
    "    pred_df = time_test.copy()\n",
    "    pred_df[\"true_value\"] = y_test\n",
    "    pred_df[\"predicted_value\"] = y_pred\n",
    "    pred_df.to_csv(f\"figures/{file.replace('.csv','')}_predictions_detailed.csv\", index=False)\n",
    "\n",
    "    # Save average prediction per hour\n",
    "    avg_df = pred_df.groupby('hour')[['true_value', 'predicted_value']].mean().reset_index()\n",
    "    avg_df.to_csv(f\"figures/{file.replace('.csv','')}_avg_hourly_predictions.csv\", index=False)\n",
    "\n",
    "# Save overall results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"figures/xgboost_24h_sliding_window_results.csv\", index=False)\n",
    "print(\"üìÅ Results saved to: figures/xgboost_24h_sliding_window_results.csv\")\n",
    "print(\"üíæ Models saved to: models/xgboost/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e032a4-7843-4f0d-876f-abe8a3015bc5",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534de54c-3dab-4fe9-b516-21b0123b0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load environmental data from CSV files\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "os.makedirs(\"models/lstm\", exist_ok=True)\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "lstm_results = []\n",
    "\n",
    "def create_lstm_sequences(df, feature_cols, target_col='non_shiftable_load', window_size=24, forecast_horizon=24):\n",
    "    X, y, meta = [], [], []\n",
    "    for i in range(window_size, len(df) - forecast_horizon):\n",
    "        x_seq = df[feature_cols].iloc[i - window_size:i].values\n",
    "        target = df[target_col].iloc[i + forecast_horizon]\n",
    "        ts = df.iloc[i + forecast_horizon][['datetime', 'month', 'day', 'hour']]\n",
    "        X.append(x_seq)\n",
    "        y.append(target)\n",
    "        meta.append(ts)\n",
    "    return np.array(X), np.array(y), pd.DataFrame(meta)\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "for file in building_files:\n",
    "    df = pd.read_csv(os.path.join(data_path, file))\n",
    "    if target_col not in df.columns:\n",
    "        continue\n",
    "\n",
    "    df = construct_datetime(df)\n",
    "    df = add_lag_and_rolling_features(df, target_col=target_col)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    feature_cols = [col for col in df.columns if col not in ['datetime', 'month', 'day', 'hour', target_col]]\n",
    "    scaler = StandardScaler()\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "    X, y, meta = create_lstm_sequences(df, feature_cols, target_col, 24, forecast_horizon)\n",
    "    split_idx = int(len(X) * 0.7)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    meta_test = meta.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "    model = build_lstm_model(X_train.shape[1:])\n",
    "    model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=5)], verbose=0)\n",
    "\n",
    "    model.save(f\"models/lstm/{file.replace('.csv','')}_lstm_model.h5\")\n",
    "\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    lstm_results.append({\n",
    "        \"Building\": file.replace(\".csv\", \"\"),\n",
    "        \"RMSE_24h\": rmse,\n",
    "        \"MAE_24h\": mae\n",
    "    })\n",
    "\n",
    "    pred_df = meta_test.copy()\n",
    "    pred_df[\"true_value\"] = y_test\n",
    "    pred_df[\"predicted_value\"] = y_pred\n",
    "    pred_df.to_csv(f\"figures/{file.replace('.csv','')}_LSTM_predictions_detailed.csv\", index=False)\n",
    "\n",
    "    avg_df = pred_df.groupby('hour')[['true_value', 'predicted_value']].mean().reset_index()\n",
    "    avg_df.to_csv(f\"figures/{file.replace('.csv','')}_LSTM_avg_hourly_predictions.csv\", index=False)\n",
    "\n",
    "pd.DataFrame(lstm_results).to_csv(\"figures/lstm_24h_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8498e8a-1cd9-4fde-a08f-69332326ebb9",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e3d04-5c10-4265-b811-2534d8a6fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load environmental data from CSV files\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "os.makedirs(\"models/random_forest\", exist_ok=True)\n",
    "rf_results = []\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "for file in building_files:\n",
    "    df = pd.read_csv(os.path.join(data_path, file))\n",
    "    if target_col not in df.columns:\n",
    "        continue\n",
    "\n",
    "    df = construct_datetime(df)\n",
    "    df = add_lag_and_rolling_features(df, target_col=target_col)\n",
    "    df['target'] = df[target_col].shift(-forecast_horizon)\n",
    "    df['datetime_shifted'] = df['datetime'].shift(-forecast_horizon)\n",
    "    df['month_shifted'] = df['month'].shift(-forecast_horizon)\n",
    "    df['day_shifted'] = df['day'].shift(-forecast_horizon)\n",
    "    df['hour_shifted'] = df['hour'].shift(-forecast_horizon)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    time_meta = df[['datetime_shifted', 'month_shifted', 'day_shifted', 'hour_shifted']].reset_index(drop=True)\n",
    "    time_meta.columns = ['datetime', 'month', 'day', 'hour']\n",
    "\n",
    "    feature_cols = [col for col in df.columns if col not in [\n",
    "        target_col, 'target', 'datetime', 'datetime_shifted',\n",
    "        'month', 'month_shifted', 'day', 'day_shifted', 'hour', 'hour_shifted'\n",
    "    ]]\n",
    "    X = df[feature_cols].values\n",
    "    y = df['target'].values\n",
    "\n",
    "    split_idx = int(len(X) * 0.7)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    time_test = time_meta.iloc[split_idx:]\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    grid = GridSearchCV(rf, param_grid_rf, cv=3, scoring='neg_mean_squared_error', verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    joblib.dump(best_model, f\"models/random_forest/{file.replace('.csv','')}_rf_model.pkl\")\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    rf_results.append({\n",
    "        \"Building\": file.replace(\".csv\", \"\"),\n",
    "        \"Best_Params\": grid.best_params_,\n",
    "        \"RMSE_24h\": rmse,\n",
    "        \"MAE_24h\": mae\n",
    "    })\n",
    "\n",
    "    pred_df = time_test.copy()\n",
    "    pred_df[\"true_value\"] = y_test\n",
    "    pred_df[\"predicted_value\"] = y_pred\n",
    "    pred_df.to_csv(f\"figures/{file.replace('.csv','')}_RF_predictions_detailed.csv\", index=False)\n",
    "\n",
    "    avg_df = pred_df.groupby('hour')[['true_value', 'predicted_value']].mean().reset_index()\n",
    "    avg_df.to_csv(f\"figures/{file.replace('.csv','')}_RF_avg_hourly_predictions.csv\", index=False)\n",
    "\n",
    "pd.DataFrame(rf_results).to_csv(\"figures/random_forest_24h_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e375192-c16b-4e07-b956-1a5a3e6fe683",
   "metadata": {},
   "source": [
    "## TL Testing per Cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a3fa6-0795-42f4-93e9-1e9667f7ebbd",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eebe1e-ed49-4086-8222-32d3d084c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load environmental data from CSV files\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Same config as training\n",
    "data_path = \"Dataset_Preprocessed\"\n",
    "model_path = \"models/xgboost\"\n",
    "forecast_horizon = 24\n",
    "target_col = 'non_shiftable_load'\n",
    "\n",
    "# Use same lag/rolling config as before\n",
    "def add_lag_and_rolling_features(df, target_col, lags=[1, 2, 3, 6, 12, 24], roll_windows=[6, 12, 24]):\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    for window in roll_windows:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "    return df\n",
    "\n",
    "def construct_datetime(df):\n",
    "    df['hour'] = df['hour'].replace(24, 0)\n",
    "    df['day'] = (df.index // 24) + 1\n",
    "    df['year'] = 2022\n",
    "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']], errors='coerce')\n",
    "    df.drop(columns=['year'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Process each model/dataset\n",
    "for filename in os.listdir(model_path):\n",
    "    if not filename.endswith(\"_xgb_model.pkl\"):\n",
    "        continue\n",
    "\n",
    "    building = filename.replace(\"_xgb_model.pkl\", \"\")\n",
    "    file_path = os.path.join(data_path, f\"{building}.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ö†Ô∏è Dataset missing for {building}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = construct_datetime(df)\n",
    "    df = add_lag_and_rolling_features(df, target_col=target_col)\n",
    "    df['target'] = df[target_col].shift(-forecast_horizon)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col, 'target', 'datetime', 'month', 'day', 'hour', 'day_type']]\n",
    "\n",
    "    # Save feature list\n",
    "    feature_out = os.path.join(model_path, f\"{building}_xgb_model_features.pkl\")\n",
    "    joblib.dump(feature_cols, feature_out)\n",
    "    print(f\"‚úÖ Saved features for {building}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293247a8-120f-4dcf-ac5f-6b73d763d710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# üì• Load environmental data from CSV files\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# --- CONFIG ---\n",
    "data_path = \"Dataset_Preprocessed\"\n",
    "results_path = \"transfer_learning\"\n",
    "original_results_path = \"figures/xgboost_24h_sliding_window_results.csv\"\n",
    "model_base_path = \"models/xgboost\"\n",
    "target_col = 'non_shiftable_load'\n",
    "forecast_horizon = 24\n",
    "timeframes = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70,\n",
    "               77, 84, 91, 98, 105, 112, 119, 126, 133, 140,\n",
    "               147, 154, 161, 168, 175, 182, 189, 196, 203, 210,\n",
    "               217, 224, 231, 238, 240]\n",
    "\n",
    "clusters = {\n",
    "    1: [15, 3, 9],\n",
    "    2: [7, 14, 2, 5, 8],\n",
    "    3: [12, 16],\n",
    "    4: [4, 13, 1, 6],\n",
    "    5: [17, 10, 11]\n",
    "}\n",
    "\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "def add_lag_and_rolling_features(df, target_col, lags=[1, 2, 3, 6, 12, 24], roll_windows=[6, 12, 24]):\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    for window in roll_windows:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "    return df\n",
    "\n",
    "def construct_datetime(df):\n",
    "    df['hour'] = df['hour'].replace(24, 0)\n",
    "    df['day'] = (df.index // 24) + 1\n",
    "    df['year'] = 2022\n",
    "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']], errors='coerce')\n",
    "    df.drop(columns=['year'], inplace=True)\n",
    "    return df\n",
    "\n",
    "for cluster_id, buildings in clusters.items():\n",
    "    print(f\"\\nüîÅ Cluster {cluster_id}\")\n",
    "    cluster_dir = os.path.join(results_path, f\"cluster_{cluster_id}\")\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "    # --- Load best model ---\n",
    "    perf_df = pd.read_csv(original_results_path)\n",
    "    best_row = perf_df[perf_df[\"Building\"].isin([f\"Building_{b}\" for b in buildings])].loc[lambda x: x[\"RMSE_24h\"].idxmin()]\n",
    "    source_building = best_row[\"Building\"]\n",
    "    best_building_id = int(source_building.split(\"_\")[1])\n",
    "\n",
    "    print(f\"üèÜ Best model: {source_building}\")\n",
    "    with open(os.path.join(cluster_dir, \"best_source_building.txt\"), \"w\") as f:\n",
    "        f.write(f\"Best: {source_building}\\n\")\n",
    "        f.write(f\"RMSE: {best_row['RMSE_24h']:.3f}, MAE: {best_row['MAE_24h']:.3f}\")\n",
    "\n",
    "    model_path = os.path.join(model_base_path, f\"{source_building}_xgb_model.pkl\")\n",
    "    feature_path = model_path.replace(\".pkl\", \"_features.pkl\")\n",
    "    base_model = joblib.load(model_path)\n",
    "    feature_cols = joblib.load(feature_path)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for target_id in buildings:\n",
    "        if target_id == best_building_id:\n",
    "            continue\n",
    "\n",
    "        print(f\"üîÅ Transfer to Building_{target_id}\")\n",
    "        df = pd.read_csv(os.path.join(data_path, f\"Building_{target_id}.csv\"))\n",
    "        df = construct_datetime(df)\n",
    "        df = add_lag_and_rolling_features(df, target_col)\n",
    "        df['target'] = df[target_col].shift(-forecast_horizon)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Load test timestamps\n",
    "        original_pred_file = os.path.join(\"figures\", f\"Building_{target_id}_predictions_detailed.csv\")\n",
    "        if not os.path.exists(original_pred_file):\n",
    "            print(f\"‚ö†Ô∏è Missing {original_pred_file}, skipping.\")\n",
    "            continue\n",
    "        original_times = pd.to_datetime(pd.read_csv(original_pred_file)[\"datetime\"])\n",
    "\n",
    "        for days in timeframes:\n",
    "            df_sub = df[df['datetime'] <= df['datetime'].min() + pd.Timedelta(days=days)].copy()\n",
    "            if len(df_sub) < 50:\n",
    "                print(f\"‚ö†Ô∏è Not enough training data for {days} days\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df_sub = df_sub[feature_cols + ['datetime', 'month', 'day', 'hour', 'target']]\n",
    "            except KeyError:\n",
    "                print(f\"‚ö†Ô∏è Feature mismatch in Building_{target_id}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Get same test timestamps as original\n",
    "            test_df = df[df['datetime'].isin(original_times)].copy()\n",
    "            train_df = df_sub.copy()\n",
    "\n",
    "            if len(test_df) == 0 or len(train_df) < 50:\n",
    "                print(f\"‚ö†Ô∏è Skipping {target_id}, {days} days ‚Äî not enough data\")\n",
    "                continue\n",
    "\n",
    "            X_train = train_df[feature_cols].values\n",
    "            y_train = train_df[\"target\"].values\n",
    "            X_test = test_df[feature_cols].values\n",
    "            y_test = test_df[\"target\"].values\n",
    "\n",
    "            # ----- Transfer Learning -----\n",
    "            model_tl = joblib.load(model_path)\n",
    "            model_tl.fit(X_train, y_train, xgb_model=model_tl.get_booster())\n",
    "            y_pred_tl = model_tl.predict(X_test)\n",
    "\n",
    "            rmse_tl = mean_squared_error(y_test, y_pred_tl, squared=False)\n",
    "            mae_tl = mean_absolute_error(y_test, y_pred_tl)\n",
    "\n",
    "            # Save TL model\n",
    "            joblib.dump(model_tl, os.path.join(cluster_dir, f\"Building_{target_id}_tuned_{days}days.pkl\"))\n",
    "\n",
    "            pred_df = test_df[['datetime', 'month', 'day', 'hour']].copy().reset_index(drop=True)\n",
    "            pred_df[\"true_value\"] = y_test\n",
    "            pred_df[\"predicted_value\"] = y_pred_tl\n",
    "            pred_df.to_csv(os.path.join(cluster_dir, f\"Building_{target_id}_predictions_detailed_{days}days_TL.csv\"), index=False)\n",
    "\n",
    "            # ----- Baseline Model -----\n",
    "            model_base = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "            model_base.fit(X_train, y_train)\n",
    "            y_pred_base = model_base.predict(X_test)\n",
    "\n",
    "            rmse_base = mean_squared_error(y_test, y_pred_base, squared=False)\n",
    "            mae_base = mean_absolute_error(y_test, y_pred_base)\n",
    "\n",
    "            # Save baseline model\n",
    "            joblib.dump(model_base, os.path.join(cluster_dir, f\"Building_{target_id}_baseline_{days}days.pkl\"))\n",
    "\n",
    "            pred_base = test_df[['datetime', 'month', 'day', 'hour']].copy().reset_index(drop=True)\n",
    "            pred_base[\"true_value\"] = y_test\n",
    "            pred_base[\"predicted_value\"] = y_pred_base\n",
    "            pred_base.to_csv(os.path.join(cluster_dir, f\"Building_{target_id}_predictions_detailed_{days}days_BASE.csv\"), index=False)\n",
    "\n",
    "            # Save combined metrics\n",
    "            results.append({\n",
    "                \"Cluster\": cluster_id,\n",
    "                \"Source\": source_building,\n",
    "                \"Target\": f\"Building_{target_id}\",\n",
    "                \"Days\": days,\n",
    "                \"RMSE_TL\": rmse_tl,\n",
    "                \"MAE_TL\": mae_tl,\n",
    "                \"RMSE_BASE\": rmse_base,\n",
    "                \"MAE_BASE\": mae_base\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(results).to_csv(os.path.join(cluster_dir, \"results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d876fa-8078-4897-baa0-bd27c1d6781e",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59456d8-0508-4f31-894a-77dfe4f90b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cff8e7-7413-492b-b31f-7cd17ed9d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load environmental data from CSV files\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# --- CONFIG ---\n",
    "data_path = \"Dataset_Preprocessed\"\n",
    "results_path = \"transfer_learning_lstm\"\n",
    "model_path_base = \"models/lstm\"\n",
    "predictions_base = \"figures\"\n",
    "forecast_horizon = 24\n",
    "window_size = 24\n",
    "target_col = 'non_shiftable_load'\n",
    "timeframes = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70,\n",
    "               77, 84, 91, 98, 105, 112, 119, 126, 133, 140,\n",
    "               147, 154, 161, 168, 175, 182, 189, 196, 203, 210,\n",
    "               217, 224, 231, 238, 240]\n",
    "\n",
    "clusters = {\n",
    "    1: [15, 3, 9],\n",
    "    2: [7, 14, 2, 5, 8],\n",
    "    3: [12, 16],\n",
    "    4: [4, 13, 1, 6],\n",
    "    5: [17, 10, 11]\n",
    "}\n",
    "\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "# --- Feature + datetime helper ---\n",
    "def construct_datetime(df):\n",
    "    df['hour'] = df['hour'].replace(24, 0)\n",
    "    df['day'] = (df.index // 24) + 1\n",
    "    df['year'] = 2022\n",
    "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']], errors='coerce')\n",
    "    df.drop(columns=['year'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_lag_and_rolling_features(df, target_col='non_shiftable_load', lags=[1,2,3,6,12,24], roll_windows=[6,12,24]):\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f\"{target_col}_lag_{lag}\"] = df[target_col].shift(lag)\n",
    "    for win in roll_windows:\n",
    "        df[f\"{target_col}_roll_mean_{win}\"] = df[target_col].rolling(win).mean()\n",
    "        df[f\"{target_col}_roll_std_{win}\"] = df[target_col].rolling(win).std()\n",
    "    return df\n",
    "\n",
    "def create_lstm_sequences(df, feature_cols, window_size=24, forecast_horizon=24):\n",
    "    X, y, meta = [], [], []\n",
    "    for i in range(window_size, len(df) - forecast_horizon):\n",
    "        x_seq = df[feature_cols].iloc[i - window_size:i].values\n",
    "        target = df[target_col].iloc[i + forecast_horizon]\n",
    "        ts = df.iloc[i + forecast_horizon][['datetime', 'month', 'day', 'hour']]\n",
    "        X.append(x_seq)\n",
    "        y.append(target)\n",
    "        meta.append(ts)\n",
    "    return np.array(X), np.array(y), pd.DataFrame(meta)\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "b_buildings = ['Building_15','Building_5','Building_12','Building_6','Building_11']\n",
    "i = 0\n",
    "# --- MAIN TL LOOP ---\n",
    "for cluster_id, buildings in clusters.items():\n",
    "    cluster_dir = os.path.join(results_path, f\"cluster_{cluster_id}\")\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "    # Identify best source model from prior results\n",
    "    perf_df = pd.read_csv(\"figures/lstm_24h_results.csv\")\n",
    "    best_row = perf_df[perf_df[\"Building\"].isin([f\"Building_{b}\" for b in buildings])].loc[lambda x: x[\"RMSE_24h\"].idxmin()]\n",
    "    source_building = b_buildings[i]  #best_row[\"Building\"] # Building_1\n",
    "    i = i + 1\n",
    "    source_model_path = f\"{model_path_base}/{source_building}_lstm_model.h5\"\n",
    "    best_building_id = int(source_building.split(\"_\")[1])\n",
    "\n",
    "    print(f\"üì¶ Cluster {cluster_id}: Using {source_building} as source\")\n",
    "\n",
    "    with open(os.path.join(cluster_dir, \"best_source_building.txt\"), \"w\") as f:\n",
    "        f.write(f\"Best: {source_building}\\nRMSE: {best_row['RMSE_24h']:.3f}, MAE: {best_row['MAE_24h']:.3f}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for target_id in buildings:\n",
    "        if target_id == best_building_id:\n",
    "            continue\n",
    "\n",
    "        target_file = f\"Building_{target_id}.csv\"\n",
    "        df = pd.read_csv(os.path.join(data_path, target_file))\n",
    "        df = construct_datetime(df)\n",
    "        df = add_lag_and_rolling_features(df, target_col)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        feature_cols = [col for col in df.columns if col not in ['datetime', 'month', 'day', 'hour', target_col, 'target']]\n",
    "        scaler = StandardScaler()\n",
    "        df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "        # Get original test timestamps\n",
    "        pred_file = f\"{predictions_base}/Building_{target_id}_LSTM_predictions_detailed.csv\"\n",
    "        test_times = pd.read_csv(pred_file)[\"datetime\"]\n",
    "        test_times = pd.to_datetime(test_times)\n",
    "\n",
    "        for days in timeframes:\n",
    "            df_sub = df[df['datetime'] <= df['datetime'].min() + pd.Timedelta(days=days)].copy()\n",
    "            if len(df_sub) < 100:\n",
    "                continue\n",
    "\n",
    "            X_all, y_all, meta_all = create_lstm_sequences(df_sub, feature_cols, window_size, forecast_horizon)\n",
    "            df_test_full = df[df[\"datetime\"].isin(test_times)]\n",
    "            X_test, y_test, meta_test = create_lstm_sequences(df_test_full, feature_cols, window_size, forecast_horizon)\n",
    "\n",
    "            # --- Transfer Learning ---\n",
    "            model_tl = load_model(source_model_path, compile=False)\n",
    "            model_tl.compile(loss='mae', optimizer='adam')\n",
    "            model_tl.fit(X_all, y_all, epochs=10, batch_size=32, verbose=0)\n",
    "            y_pred_tl = model_tl.predict(X_test).flatten()\n",
    "            rmse_tl = mean_squared_error(y_test, y_pred_tl, squared=False)\n",
    "            mae_tl = mean_absolute_error(y_test, y_pred_tl)\n",
    "\n",
    "            # --- Reference Model ---\n",
    "            model_ref = build_lstm_model(X_all.shape[1:])\n",
    "            model_ref.fit(X_all, y_all, epochs=30, batch_size=32, verbose=0)\n",
    "            y_pred_base = model_ref.predict(X_test).flatten()\n",
    "            rmse_base = mean_squared_error(y_test, y_pred_base, squared=False)\n",
    "            mae_base = mean_absolute_error(y_test, y_pred_base)\n",
    "\n",
    "            # Save predictions\n",
    "            meta_test[\"true_value\"] = y_test\n",
    "            meta_test[\"predicted_value_tl\"] = y_pred_tl\n",
    "            meta_test[\"predicted_value_base\"] = y_pred_base\n",
    "            meta_test.to_csv(os.path.join(cluster_dir, f\"Building_{target_id}_predictions_{days}days.csv\"), index=False)\n",
    "\n",
    "            # Save models\n",
    "            model_tl.save(os.path.join(cluster_dir, f\"Building_{target_id}_tl_{days}days.h5\"))\n",
    "            model_ref.save(os.path.join(cluster_dir, f\"Building_{target_id}_base_{days}days.h5\"))\n",
    "\n",
    "            results.append({\n",
    "                \"Cluster\": cluster_id,\n",
    "                \"Source\": source_building,\n",
    "                \"Target\": f\"Building_{target_id}\",\n",
    "                \"Days\": days,\n",
    "                \"RMSE_TL\": rmse_tl,\n",
    "                \"MAE_TL\": mae_tl,\n",
    "                \"RMSE_BASE\": rmse_base,\n",
    "                \"MAE_BASE\": mae_base\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(results).to_csv(os.path.join(cluster_dir, \"results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b679d07-cb1a-458e-a705-9ab6f4bdf06b",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec52c3-7a85-49c8-9e7e-1feb14dff880",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27e344-62e1-4bc1-a943-e45141cb9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load environmental data from CSV files\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# CONFIG\n",
    "data_path = \"Dataset_Preprocessed\"\n",
    "model_path_base = \"models/random_forest\"\n",
    "results_path = \"transfer_learning_random_forest\"\n",
    "forecast_horizon = 24\n",
    "target_col = 'non_shiftable_load'\n",
    "timeframes = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70,\n",
    "               77, 84, 91, 98, 105, 112, 119, 126, 133, 140,\n",
    "               147, 154, 161, 168, 175, 182, 189, 196, 203, 210,\n",
    "               217, 224, 231, 238, 240]\n",
    "\n",
    "clusters = {\n",
    "    1: [15, 3, 9],\n",
    "    2: [7, 14, 2, 5, 8],\n",
    "    3: [12, 16],\n",
    "    4: [4, 13, 1, 6],\n",
    "    5: [17, 10, 11]\n",
    "}\n",
    "\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "def construct_datetime(df):\n",
    "    df['hour'] = df['hour'].replace(24, 0)\n",
    "    df['day'] = (df.index // 24) + 1\n",
    "    df['year'] = 2022\n",
    "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']], errors='coerce')\n",
    "    df.drop(columns=['year'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_lag_and_rolling_features(df, target_col, lags=[1, 2, 3, 6, 12, 24], roll_windows=[6, 12, 24]):\n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    for window in roll_windows:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "    return df\n",
    "\n",
    "for cluster_id, buildings in clusters.items():\n",
    "    cluster_dir = os.path.join(results_path, f\"cluster_{cluster_id}\")\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "    # Identify best model from figures\n",
    "    perf_df = pd.read_csv(\"figures/random_forest_24h_results.csv\")\n",
    "    best_row = perf_df[perf_df[\"Building\"].isin([f\"Building_{b}\" for b in buildings])].loc[lambda x: x[\"RMSE_24h\"].idxmin()]\n",
    "    source_building = best_row[\"Building\"]\n",
    "    best_building_id = int(source_building.split(\"_\")[1])\n",
    "\n",
    "    with open(os.path.join(cluster_dir, \"best_source_building.txt\"), \"w\") as f:\n",
    "        f.write(f\"Best: {source_building}\\n\")\n",
    "        f.write(f\"RMSE: {best_row['RMSE_24h']:.3f}, MAE: {best_row['MAE_24h']:.3f}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for target_id in buildings:\n",
    "        if target_id == best_building_id:\n",
    "            continue\n",
    "\n",
    "        target_file = f\"Building_{target_id}.csv\"\n",
    "        df = pd.read_csv(os.path.join(data_path, target_file))\n",
    "        df = construct_datetime(df)\n",
    "        df = add_lag_and_rolling_features(df, target_col)\n",
    "        df['target'] = df[target_col].shift(-forecast_horizon)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        feature_cols = [col for col in df.columns if col not in ['datetime', 'month', 'day', 'hour', 'target', target_col]]\n",
    "\n",
    "        # Load test timestamps from full prediction\n",
    "        pred_file = f\"figures/Building_{target_id}_RF_predictions_detailed.csv\"\n",
    "        if not os.path.exists(pred_file):\n",
    "            print(f\"‚ö†Ô∏è Missing {pred_file}, skipping {target_file}\")\n",
    "            continue\n",
    "        test_times = pd.to_datetime(pd.read_csv(pred_file)[\"datetime\"])\n",
    "\n",
    "        for days in timeframes:\n",
    "            df_train = df[df['datetime'] <= df['datetime'].min() + pd.Timedelta(days=days)].copy()\n",
    "            df_test = df[df[\"datetime\"].isin(test_times)].copy()\n",
    "\n",
    "            if len(df_train) < 50 or len(df_test) < 10:\n",
    "                continue\n",
    "\n",
    "            X_train = df_train[feature_cols].values\n",
    "            y_train = df_train['target'].values\n",
    "            X_test = df_test[feature_cols].values\n",
    "            y_test = df_test['target'].values\n",
    "\n",
    "            # --- Transfer Learning ---\n",
    "            rf_tl = joblib.load(f\"{model_path_base}/{source_building}_rf_model.pkl\")\n",
    "            rf_tl.fit(X_train, y_train)\n",
    "            y_pred_tl = rf_tl.predict(X_test)\n",
    "            rmse_tl = mean_squared_error(y_test, y_pred_tl, squared=False)\n",
    "            mae_tl = mean_absolute_error(y_test, y_pred_tl)\n",
    "\n",
    "            joblib.dump(rf_tl, os.path.join(cluster_dir, f\"Building_{target_id}_rf_model_{days}days_TL.pkl\"))\n",
    "\n",
    "            pred_df_tl = df_test[['datetime', 'month', 'day', 'hour']].copy().reset_index(drop=True)\n",
    "            pred_df_tl[\"true_value\"] = y_test\n",
    "            pred_df_tl[\"predicted_value\"] = y_pred_tl\n",
    "            pred_df_tl.to_csv(os.path.join(cluster_dir, f\"Building_{target_id}_predictions_detailed_{days}days_TL.csv\"), index=False)\n",
    "\n",
    "            # --- Reference Model ---\n",
    "            rf_base = RandomForestRegressor(random_state=42)\n",
    "            rf_base.fit(X_train, y_train)\n",
    "            y_pred_base = rf_base.predict(X_test)\n",
    "            rmse_base = mean_squared_error(y_test, y_pred_base, squared=False)\n",
    "            mae_base = mean_absolute_error(y_test, y_pred_base)\n",
    "\n",
    "            joblib.dump(rf_base, os.path.join(cluster_dir, f\"Building_{target_id}_rf_model_{days}days_BASE.pkl\"))\n",
    "\n",
    "            pred_df_base = df_test[['datetime', 'month', 'day', 'hour']].copy().reset_index(drop=True)\n",
    "            pred_df_base[\"true_value\"] = y_test\n",
    "            pred_df_base[\"predicted_value\"] = y_pred_base\n",
    "            pred_df_base.to_csv(os.path.join(cluster_dir, f\"Building_{target_id}_predictions_detailed_{days}days_BASE.csv\"), index=False)\n",
    "\n",
    "            # Save results\n",
    "            results.append({\n",
    "                \"Cluster\": cluster_id,\n",
    "                \"Source\": source_building,\n",
    "                \"Target\": f\"Building_{target_id}\",\n",
    "                \"Days\": days,\n",
    "                \"RMSE_TL\": rmse_tl,\n",
    "                \"MAE_TL\": mae_tl,\n",
    "                \"RMSE_BASE\": rmse_base,\n",
    "                \"MAE_BASE\": mae_base\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(results).to_csv(os.path.join(cluster_dir, \"results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e81859-cc33-411d-abce-c07da9afb0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a9b5a-e80c-4fb9-927e-6a1d93b68d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load environmental data from CSV files\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# MAPE helper\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero = y_true != 0\n",
    "    return np.mean(np.abs((y_true[non_zero] - y_pred[non_zero]) / y_true[non_zero])) * 100\n",
    "\n",
    "# Path to root folder with clusters\n",
    "root_dir = \"transfer_learning\"\n",
    "\n",
    "# Storage for metrics and raw values\n",
    "metrics = []\n",
    "raw_rows = []\n",
    "\n",
    "# Walk through clusters\n",
    "for cluster in sorted(os.listdir(root_dir)):\n",
    "    cluster_path = os.path.join(root_dir, cluster)\n",
    "    if not os.path.isdir(cluster_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"üìÅ Processing: {cluster}\")\n",
    "\n",
    "    for file in os.listdir(cluster_path):\n",
    "        if file.endswith(\".csv\") and \"predictions_detailed\" in file:\n",
    "            filepath = os.path.join(cluster_path, file)\n",
    "            match = re.match(r\"Building_(\\d+)_predictions_detailed_(\\d+)days\\.csv\", file)\n",
    "            if not match:\n",
    "                print(f\"‚ö†Ô∏è Skipped: {file}\")\n",
    "                continue\n",
    "\n",
    "            building_id = int(match.group(1))\n",
    "            days = int(match.group(2))\n",
    "\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            if 'true_value' not in df.columns or 'predicted_value' not in df.columns:\n",
    "                print(f\"‚ö†Ô∏è Missing columns in {file}\")\n",
    "                continue\n",
    "\n",
    "            # Metrics\n",
    "            y_true = df[\"true_value\"]\n",
    "            y_pred = df[\"predicted_value\"]\n",
    "            rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "            metrics.append({\n",
    "                \"Cluster\": cluster,\n",
    "                \"Building\": f\"Building_{building_id}\",\n",
    "                \"Days\": days,\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"MAPE\": mape\n",
    "            })\n",
    "\n",
    "            # Raw values\n",
    "            for i, row in df.iterrows():\n",
    "                raw_rows.append({\n",
    "                    \"Cluster\": cluster,\n",
    "                    \"Building\": f\"Building_{building_id}\",\n",
    "                    \"Days\": days,\n",
    "                    \"Datetime\": row.get(\"datetime\", None),\n",
    "                    \"Hour\": row.get(\"hour\", None),\n",
    "                    \"True_Value\": row[\"true_value\"],\n",
    "                    \"Predicted_Value\": row[\"predicted_value\"]\n",
    "                })\n",
    "\n",
    "# ---- Export ----\n",
    "\n",
    "if metrics:\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    try:\n",
    "        metrics_df.sort_values(by=[\"Cluster\", \"Building\", \"Days\"], inplace=True)\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Sort error: {e}\")\n",
    "        print(\"üîç Columns:\", metrics_df.columns)\n",
    "\n",
    "    metrics_df.to_csv(\"xgboost_metrics_summary.csv\", index=False)\n",
    "    print(\"‚úÖ Saved: xgboost_metrics_summary.csv\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No metrics collected!\")\n",
    "\n",
    "if raw_rows:\n",
    "    raw_df = pd.DataFrame(raw_rows)\n",
    "    raw_df.to_csv(\"xgboost_raw_predictions.csv\", index=False)\n",
    "    print(\"‚úÖ Saved: xgboost_raw_predictions.csv\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No raw prediction data collected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9522ed9-f9a3-4383-9030-7d160f3b2aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e20d8-733e-4143-adfa-0290c83cc0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0120b-2b6e-4a18-8301-d13bb8e17371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e611ed-e2ff-4aee-85c6-4e3e067501d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2bb601-14b0-4c00-a01e-e7209773fefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6224d38-077e-45ab-bdb2-ad8b6415c4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476e116-54c2-4f06-8fa1-d65068770eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbb74350-bc8f-41a3-b110-fc9f53953aa2",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9014ab6-d0d2-4e45-b0a8-9ff13f98f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check NaNs\n",
    "def check_nans(dfs, names):\n",
    "    for df, name in zip(dfs, names):\n",
    "        print(f\"NaN check for {name}:\\n\", df.isna().sum(), \"\\n\")\n",
    "\n",
    "# Function to detect outliers using IQR\n",
    "def detect_outliers(df):\n",
    "    outliers = {}\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers[col] = ((df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)).sum()\n",
    "    return outliers\n",
    "\n",
    "# Function to plot heatmaps\n",
    "def plot_heatmap(df, name):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df.corr(), annot=False, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(f\"Correlation Heatmap - {name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/ELAD22/Plots_After_Cleaning/Heatmaps/heatmap_{name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Function to plot each numeric column safely\n",
    "def plot_each_column(df, name):\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        plt.figure()\n",
    "        df[col].plot()\n",
    "        plt.title(f\"{col} - {name}\")\n",
    "        plt.xlabel(\"Timestep\")\n",
    "        plt.ylabel(col)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Sanitize filename\n",
    "        safe_col = re.sub(r'[^\\w\\-_. ]', '_', col).replace(\" \", \"_\")\n",
    "        filename = f\"figures/ELAD22/Plots_After_Cleaning/Plots/{name}_{safe_col}.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26eeb1-7f7c-40ab-8c77-8611c4ad7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_impute(df, outlier_threshold=0.5, nan_threshold=0.5):\n",
    "    df = df.copy()\n",
    "    num_rows = len(df)\n",
    "\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    # Step 1: Identify and count outliers\n",
    "    outlier_flags = pd.DataFrame(False, index=df.index, columns=numeric_cols)\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)\n",
    "        outlier_flags[col] = outlier_mask\n",
    "\n",
    "    # Step 2: Drop columns with too many NaNs or outliers\n",
    "    columns_to_drop = []\n",
    "    for col in numeric_cols:\n",
    "        nan_ratio = df[col].isna().sum() / num_rows\n",
    "        outlier_ratio = outlier_flags[col].sum() / num_rows\n",
    "        if nan_ratio > nan_threshold or outlier_ratio > outlier_threshold:\n",
    "            columns_to_drop.append(col)\n",
    "\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    # Step 3: Impute remaining NaNs with forward fill + backward fill + median fallback\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if df[col].isna().any():\n",
    "            df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "            if df[col].isna().any():\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c902d4-1b58-48a4-b557-cc15ae1d40c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d54b7-3273-4b8f-aea6-615b094d6df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf01e5-828d-40d0-adf3-0fc4207e0444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
